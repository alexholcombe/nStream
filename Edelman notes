This is an exciting and thought provoking article! It seems that all the motivations for buffering here boil down to the idea that higher level operations (action, prediction) are complex and thus need steady (held/buffered) low level input. One idea this inspires is that our experiments measure some fundamental time interval that stimuli must be buffered for in order to inform higher level operations. Perhaps this hold is necessary for the interpretation of rapid visual events, no matter their kind. Alternatively, it might be that our stimuli are held for some operation associated with reading. Visual words may need to be held in order to access phonological or semantic information (Freeman, 2015, has a heirarchical theory of cortical processing that includes a meaning level, could be useful here). Using letter stimuli might initiate this sort of hold pattern. One way to test this is to alter the stimuli and look at changes in parameter estimates (this would require a bit of training). 
	- Another way to manipulate the hold time would be to vary the task. Different tasks require different computations
	- The computational complexity of a task determines the hold time for task-relevant input
		- Is this only true for serial processing?
	- need to be careful that this theory doesn't require accessing the stimulus category before recognising the stimulus. 
	- Periodicity in neural signals is consistent with sampling-and-holding. There must be some way to reset the cycle - so that important information can inform perception (they're unclear about this last point. I imputed it. Could be wrong). Can we reset the periodicity of our buffer somehow? Repeat the cued letter? Would that cause RB, or would the dynamics of the system change so that 
	- Alpha band oscillations are associated with perceptual experience (phase and interaction with other oscillations) and the AB. Were all our SOA manipulations within the alpha range? What happens we when deviate from it?